---
title: "DAY 1 LINEAR REGRESSION"
author: "Tamas Nagy"
date: '2022-07-12'
output: 
  html_document:
   theme: spacelab
   code_download: true
   toc: true
   toc_float: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, results='hide'}
# Sets global chunk options
knitr::opts_chunk$set(echo = TRUE)

# Load these packages
suppressPackageStartupMessages(library(tidyverse))
library(broom)
library(performance)
library(sjPlot)

# Setting the default ggplot2 theme
theme_set(theme_light()) 

```

```{r}
# Read the cocktails dataset
cocktails <- read_tsv(here::here("data/cocktail_data.tsv"))

```


# Using the lm() formula

Doing a regression in R is super easy. Just use the lm() function.
The first argument is the formula, thus if you want to pipe something in lm()
you need to expicitly name the data argument. E.g. data %>% lm(y ~ x, data = .)

The **formula** is symbolic expression that defines a model.

It takes a format like this:

`y ~ 1 + x + z*w - z:w`

The elements of the formula are:  
- `~` : on the left hand side, there is the outcome (y), on the right hand, the predictors.  
- `1` : refers to the intercept that does not have to be written out as it is added by default. If you want to remove the intercept, you have to use 0 instead of 1.  
- `+` : you can add predictors using the + sign.  
- `*` : means to take two (or more) pradictors, and use their main effect AND also their interaction. `z*w` translates into `z + w + z:w`.  
- `:` : referes to an interaction, without the main effects of the predictors.  
- `-` : removes a predictor. For e.g. `z*w - z:w` translates into: `z + w`

Creating a simple linear regression of cocktail acidity on alcohol content.

```{r}
lm(abv ~ acid, data = cocktails)
```

This returns just the formula, the estimated intercept and the slope of the model.
However a lot is stored in the lm object. To be able to get them, store it in a variable.

```{r}
acid_lm <- lm(abv ~ acid, data = cocktails)
```

The summary function shows the most important output from the model that are needed for reporting.

```{r}
summary(acid_lm)
```

This also works without storing the results. However when you use pipes, mind that in lm(), data is not the first parameter

```{r}
cocktails %>% 
  lm(abv ~ acid, data = .) %>% 
  summary()
```

## Plotting a linear model
Plot the linear regression using the bulit in modeling of ggplot2

```{r}
cocktails %>% 
  ggplot() +
  aes(y = abv, x = acid) +
  geom_point() +
  geom_smooth(method = "lm") 
```

The lm object is a large list, with lots of special information that we only need occasionally. 

## Getting clean results

To get clean results, use the broom package.
The broom package creates tidy tibbles from the most important information of statistical result objects. It works for most regression types and other statistical outputs. 
The broom package has three important functions.

- tidy() returns the model summary about the parameters in a neat data frame.

```{r}
tidy(acid_lm)
```

- glance() returns important model performance metrics.

```{r}
glance(acid_lm)
```

- augment() adds important new columns to your data frames, such as the residuals (.resid), and predicted values corresponding your independent variables. These can be useful for residual diagnostics, and plotting model predictions.Ã­

```{r}
augment(acid_lm)
```

## Getting standardized coefficients

To get the standardized coefficients (scale free), you need to standardize the output and predictor variables. Use the scale() function on all variables in the model
 
```{r}
acid_lm_std <- lm(scale(abv) ~ scale(acid), data = cocktails)
summary(acid_lm_std)
```
 
You can check that the slope of acid now matches the correlation between abv and acid. Mind that correlation and linear regression finds the same slope in a different way!

```{r}
cor(cocktails$abv, cocktails$acid)
```


## Predicting values based on the model

Create predictions with new data. predict() returns a vector of predictions
```{r}
newdata <- tibble(acid = c(0.2, 0.3, 0.4))
predict(acid_lm, newdata)
```

## Updating the model 

Add more predictors using the update function
You don't need to write out all things in your formula. By using the . you can tell the function to leave everything as it was. All other parts of the formula work the same way.
Note: your original data is also kept in the lm object, so you don't even need to define that!


```{r}
new_mod <- update(acid_lm, . ~ . + sugar)
```

# Checking the assumptions for linear regression

There are multiple ways to plot and investigate model assumptions in R
I prefer to use solutions that make everything in one go.
To explore the residuals, the performance package makes diagnostic plots, using the `check_model()` function.

```{r}
library(performance)
```

Sidenote: the performance package is part of the easystats package family, that is geared towards making the reporting of statistics more seamless and standardized. It can save a lot of time to use these packages!
To install easystats, use the following line. Mind that easystats is not from CRAN!

```{r}
# install.packages("easystats", repos = "https://easystats.r-universe.dev")
# easystats::install_suggested() # This will take a lot of time!!
```

This will plot 5 different diagnostic plots that are all useful to tell if the prediction is reliable
See explanation on the slides.

```{r fig.height=9, fig.width=8}
check_model(new_mod)
```

To check heteroscetasticity inspect the residual diagnostic plots.
Multicollinearity is also displayed if you have more than one predictor in your model.
Measure multicollinearity using the variance inflaction factor (VIF)
Values for any variable should not be larger than 5.

```{r}
car::vif(new_mod)
```

Measuring the independence of residuals

```{r}
car::dwt(new_mod)
```

The homoscedasticity assumption can also be checked using the Bausch-Pagan test from the `lmtest` package.


```{r}
lmtest::bptest(new_mod)

```

It seems like model has some significant autocorrelation, so the residuals are not independent.

You can also pull out the residuals from the model and check the normality of residuals by running a normality test.

```{r}
augment(new_mod) %>% 
  pull(.resid) %>% 
  shapiro.test()
```

The Shapiro-Wilks test shows that the residuals are normally distributed.

Let's store the diagnostic values in a variable

```{r}
acid_lm_diag <- augment(new_mod)
```

# Using multiple predictions 

Let's create multiple modelts with several predictors

```{r}
lm1 <- lm(abv ~ acid + sugar, data = cocktails)
lm2 <- lm(abv ~ acid * sugar, data = cocktails)
lm3 <- lm(abv ~ acid : sugar, data = cocktails)
```

Get the confidence intervals for parameters
```{r}
confint(lm1, level = 0.95)
```

Preferably, you can also get the confint using `broom::tidy()`

```{r}
tidy(lm1, conf.int = TRUE, conf.level = .95)

```

# Using categorical predictors

R can also deal with categorical variables, as they are automatically dummied, and the first level is taken as baseline.

```{r}
lm_cat <- lm(abv ~ type, data = cocktails)
```

## Changing the baseline  

To change the baseline, you need to convert the character type variable to factor and change the factor levels to set the baseline. E.g. change it to carbonated. I recommend using the tidyverse `fct_relevel()` function from the `forcats` package (it is automatically loaded when you load tidyverse).

```{r}
lm4 <- 
  cocktails %>% 
  mutate(type = fct_relevel(type, "carbonated")) %>% 
  lm(abv ~ acid : sugar + type, data = .)
```

Let's check the model

```{r}
tidy(lm4)
```


## Post-hoc analysis

We can calculate the marginal means, and check if the difference between the categorical levels is significant. 
The `estimate_contrasts()` function from the  `{modelbased}` package can be used for both post-hoc tests and planned contrasts. For the 6 levels, we have to do `(6*5/2)` = 15 pairwise tests.
The p value has to be adjusted for multiple comparisons (FDR correction). 

```{r}
library(modelbased)
# Function name a bit misleading, you get post-hoc tests!
estimate_contrasts(lm4, contrast = "type", adjust = "fdr")
```

## Using contrasts

There are different types of contrasts to be set.
Treatment contrast is the default, it is comparing the base level to the others. If we use this, the model intercept will be equal to the mean of the baseline category.

R comes with a variety of functions that can generate different kinds of contrast matrices. For example, the table shown above is a matrix of treatment contrasts for a factor that has 6 levels.

Treatment contrast matrix
```{r}

contr.treatment(n = 6)
```

```{r}
lm_cat <- lm(abv ~ type, data = cocktails,
             contrasts = list(type = contr.treatment(n = 6, base = 1)))

summary(lm_cat)
```

In situations where no clear baseline category can be established, it may make more sense to compare each group to the mean of the other groups. This is where Helmert contrasts, generated by the `contr.helmert()` function, can be useful. The idea behind Helmert contrasts is to compare each group to the mean of the "previous" ones. That is, the first contrast represents the difference between group 2 and group 1, the second contrast represents the difference between group 3 and the mean of groups 1 and 2, and so on. One useful thing about Helmert contrasts is that every contrast sums to zero (i.e., all the columns sum to zero). This has the consequence that, the intercept term corresponds to the grand mean.

Helmert contrast matrix
```{r}
contr.helmert(n = 6)
```

```{r}
lm_cat <- lm(abv ~ type, data = cocktails,
             contrasts = list(type = contr.helmert(n = 6)))

summary(lm_cat)
```

The third option are "sum to zero" contrasts, which are used to construct pairwise comparisons between groups. Specifically, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the last group.
Like Helmert contrasts, we see that each column sums to zero, which means that the intercept term corresponds to the grand mean in the regression model. When interpreting these contrasts, each of these contrasts is a pairwise comparison between group 6 and one of the other five groups. Specifically, contrast 1 corresponds to a "group 1 minus group 6" comparison, contrast 2 corresponds to a "group 2 minus group 6" comparison, and so on.

Sum to zero contrast matrix
```{r}
contr.sum(n = 6)
```

```{r}
lm_cat <- lm(abv ~ type, data = cocktails,
             contrasts = list(type = contr.sum(n = 6)))

summary(lm_cat)

```

# Model selection

You can compare models if you use the same data and same estimation method (e.g. OLS in the case of regression).  
There are 3 widely-used metrics, all provided in `broom::glance()`
All of them have the similar underlying principle.  
You could use R2 or adj. R2 for model comparison, but those are usually not sensitive enough, and/or can be biased.  
It is preferable to use the smallest AIC and BIC value. If there are less than two units of difference between models, then the models can be considered equally good/bad. In this case, the rule of parsimony should be applied, meaning that you should prefer the simpler model, i.e. that has the fewer predictors (model df).


```{r}
glance(lm4)
```

You can also compare the logLik of models using the anova() function. It returns an F value, which is significant if there is a difference.

```{r}
anova(lm1, lm3)
```

This tells us that the more complicated model is not significantly better, so we should not use it.  
You can have more then 2 models, and the comparison refers to the _PREVIOUS_ model (so not the baseline). Pair-wise comparisons are thus preferable.

```{r}
anova(lm1, lm2, lm3, lm4)
```

Based on the comparisons, there is no significant difference. So we should choose the simplest model, that has the smallest df! It is model number 3!  

To report the results of regression, you have to use a table, according to APA. To create such a table, the easiest is to use the sjPlot package, that collects all information from the models (predictors and model summary), and creates a nice table.  

To get the table in the console, use the type = "text" argument.
```{r}
library(sjPlot)
tab_model(lm1, lm2, title = "Results")

```

You can also have the table in different formats, e.g. html. If you do this, you can save the object and view the results using your web browser. We will later learn a way to include those tables to your manuscripts.  

Let's also add standardized coefficients

```{r}
results_table_html <-
  tab_model(lm1,lm2,lm3,lm4, 
            show.std = TRUE, 
            show.est = FALSE 
  )

results_table_html
```

You can also specify the file name in the function, which will write the results in a file. 

```{r}
# tab_model(lm1,lm2,lm3,lm4, 
#           show.std = TRUE, 
#           show.est = FALSE,
#           file = "cocktail_models.html")
```

Or you can write the results manually into a file.

```{r}
# write_file(results_table_html, "results_table.html")
```






