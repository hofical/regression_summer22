---
title: "DAY 2 - REGRESSION TIPS AND TRICKS"
author: "Tamas Nagy"
date: '2022-07-13'
output: 
  html_document:
   theme: spacelab
   code_download: true
   toc: true
   toc_float: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, results='hide'}
# Sets global chunk options
knitr::opts_chunk$set(echo = TRUE)

# Load these packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(sjPlot))
library(broom)
library(performance)

# Setting the default ggplot2 theme
theme_set(theme_light()) 

```

We first load the data and rebuild all models from previous day.
```{r}
cocktails <- read_tsv(here("data/cocktail_data.tsv"))

lm1 <- lm(abv ~ acid + sugar, data = cocktails)
lm2 <- lm(abv ~ acid * sugar, data = cocktails)
lm3 <- lm(abv ~ acid : sugar, data = cocktails)
lm4 <- 
  cocktails %>% 
  mutate(type = fct_relevel(type, "carbonated")) %>% 
  lm(abv ~ acid : sugar + type, data = .)

```


# Dealing with assumption breaches 

## Multicollinearity

### Data multicollinearity 

For data multicollinearity, you can choose to:  
- use only one of the correlating predictors
- or use some data aggregating method, e.g. PCA to aggregate similar variables. 

### Structural multicollinearity 

Happens if you use the same variable in different forms in your model. This happens with interactions, and with transformations of the variable (e.g. using polynomials, see later). Structural multicollinearity only happens with continuous variables, so you usually don't need to do anything with categorical variables. 

For structural multicollinearity, you need to center (or standardize) your variables (standardization = centering + scaling).

```{r}
check_collinearity(lm2)

# Let's standardize the predictors

model_std <- lm(abv ~ scale(sugar) * scale(acid), data = cocktails)
check_collinearity(model_std)

```

Multicollinearity went away, and we didn't have to take out anything!

## Non-linearity of the relationship

If we suspect that the relationship between our outcome and predictor is not linear, we can use polynomials. This means that we also include x^2 as a predictor. However, there are two problems with this. One is that by using the original variable and it's second exponential will introduce structural multicollinearity.

```{r}
# Generate random normal numbers with a mean of 10 and sd 1
set.seed(123) # random seed for reproducibility
x <- rnorm(10, mean = 10, sd = 1)
x
# What happens with x if we just raise to the power 2 vs. using poly(x, 2)?
x^2

# The poly() function returns all polynomials up to the specified number
poly(x, 2)[,2] # We only look at the second order polynomial for now

# Compare this to the numbers that are standardized and raised to power 2.
as.numeric(scale(x)^2)
```

The difference is that poly creates so called "orthogonal" polynomials instead of raw polynomials. https://en.wikipedia.org/wiki/Orthogonal_polynomials
It is better to use orthogonal polynomials than raw ones.

## Heteroscedasticity

One of the easiest way to calculate hereoscedasticity consistent standard errors is to use the `vcov.type` argument in the `sjPlot::tab_model()` function.
There are several different robust estimation types. See help file for details.

```{r}
# Use heteroscedasticity consistent standard errors.
tab_model(model_std, vcov.type = "HC3")

```



## Normality of residuals
One of the ways to deal with normality breaches if you transform the outcome or predictor variable. Rank transform or log transform can often turn the distribution of residuals to normal.  
However, mind that when you use a transformation, the interpretation of the predictor parameters will change. For e.g. when you use rank(), the slope value will refer to the unit change in x affecting the _rank_ of the outcome.  
Often, it can be useful to choose a log value that is easier to interpret in everyday terms. Using log2() on the predictor show how much the outcome will change if the value of the predictor doubles. Mind that the base of the log is irrelevant in terms of the resulting. distribution!

```{r}
# Try these two things to generate a log normal distribution, than 
x_ln <- rlnorm(1000)
qplot(x_ln)

# This is natural log
log(x_ln) %>% 
  qplot()

# This is log2. They yield the same distribution!
log2(x_ln) %>% 
  qplot()
```

On the other hand, the rank transform will make the distribution to be uniform. This in itself is not necessarily a problem for the distribution of residuals in lm!

```{r}
rank(x_ln) %>% 
  qplot()
```

You can also use bootstrapping to estimate the residuals. There are several ways to do this. For e.g. the `parameters::parameters()` and `sjPlot::tab_model()` function has a bootstrap argument.

```{r}
library(parameters)
tab_model(lm1, bootstrap = TRUE)
# Note: sjPlot may have problems with the newest R version. The problem is solved in the github version of the package, so install it:
#devtools::install_github("strengejacke/sjPlot")

parameters(lm1, bootstrap = TRUE) %>% 
  print_html()
```

You can also get bootstrapped confidence intervals using the `rsample::reg_intervals()` function. Note that because of the random nature of bootstrapping, the values in the tables will not be exactly the same!

```{r}
library(rsample)
set.seed(134) # Setting random seed for reproducibility

reg_intervals(abv ~ acid + sugar, 
              data = cocktails, 
              model_fn = "lm",
              times = 1000)

```

You can also use "robust regression". Instead of OLS, robust regression uses iterated re-weighted least squares (IRLS) estimation. It is especially useful for smaller datasets with outliers and influential cases.

```{r}
# We don't load the MASS package as it overwrites some functions.
MASS::rlm(abv ~ acid + sugar, data = cocktails) %>% 
  summary()
```

## Dealing with outliers 

Outliers can only influence your model if you have a small dataset. In general, you should only remove outliers and influential cases if you can prove that the data points are erroneous. You cannot simply remove outliers because they don't fit your model.

You can single out observations with the slice() function

```{r}
cocktails %>% 
  slice(c(9, 41, 42, 44, 45))
```

Or use the `update()` function to change the dataset underneath the model.
We can rerun the lm without cases that have zero acid

```{r fig.height=9, fig.width=8}

acid_lm_clean <-
  cocktails %>% 
  filter(acid != 0) %>% 
  update(lm1, data = .)

summary(acid_lm_clean)

check_model(acid_lm_clean)
```

We can see that the distribution of residuals are still not perfect, which makes the reliability of the model shaky.

We can also check the dfbeta and DFFit values to see how much the model changes when we remove a case

```{r}
dfbeta(lm1) %>% head() # Change in model parameters
dffits(lm1) %>% head() # Change in residual
```

## Non-independent observations
Non-independence can take multiple forms. When your data is nested (e.g. you measure the math performance of students in a class in a school), you have a nested dependence. When you have multiple measurements from the same person, you have a crossed dependence. In general, these cases should be handled by using Linear Mixed-Effect Models (LMEMs). 
  However, by calculating aggregated statistics, you may be able to analyze the data using linear regression.
  For example, if you have a pre-post within subject design, you can use the change scores (i.e. post - pre values), and use it as an outcome variable in lm. 
  
For analyzing pre-post designs, you can either use the "Difference-in-Differences approach, or use the baseline value as a covariate. https://learncuriously.wordpress.com/2021/06/18/pre-post-analysis/ 

## Difference in differences approach

Subtract the pre value from the post value will yield a change score. We can use this as the outcome. We are not including any predictors, and investigate if the the intercept is different from zero.

```{r}
# Dataset about effects of reality TV watching on self-esteem (source: https://teachpsychscience.org/)

reality_raw <- read_csv(here("data/reality.csv"))

reality <-
  reality_raw %>% 
  mutate(se_change = se_posttest - se_pretest)

lm(se_change ~ 1, data = reality) %>% 
  summary()

```

## The outcome is not continuous  

Unless you can make your outcome variable continuous, you cannot use linear regression. You should use Generalized Linear Regression (see later material)


# Regularized regression (LASSO)

For exploratory data analysis, machine learning methods provide the most robust results. We will use LASSO regression to predict which predictors (features) are important for predicting the outcome. LASSO stands for least absolute shrinkage and selection operator.

We use the Office dataset, that contains data on the TV show Office episodes. 
We will predict the imdb rating of the episodes, based on: 
- season (certain seasons are better then others?)
- episode (e.g. earlier or later episodes are better?)
- the number of lines each character says in the episode
- the writers's name (dummied)
- the directors's name (dummied)

We will use the `{tidymodels}` infrastructure to conduct a LASSO regression.
(source: https://juliasilge.com/blog/lasso-the-office/, data source: https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-03-17)


```{r}
suppressPackageStartupMessages(library(tidymodels))

# Loading data
office <- read_csv(here("data/office.csv"))

office
```

## Splitting data into training and testing sets

The first step in an exploratory analysis is to assure that that we put aside a part of the data, that we can use for evaluation. We use the remainder for training the model.
Splitting is stratified by season, so seasons are equally represented in both datasets.

```{r}

set.seed(2022)
# Splitting the data into training and test sets
office_split <- initial_split(office, strata = season)
office_train <- training(office_split)
office_test <- testing(office_split)
```

## Creating a pre-processing recipe

We create a pre-processing blueprint, that can be applied to both the train and the test sets. This is just a blueprint, thus this pipe does not return a dataset.

```{r}

office_rec <- 
  recipe(imdb_rating ~ ., data = office_train) %>%
  # Specify variable role, so it is not included as a predictor
  update_role(episode_name, new_role = "ID") %>%
  # Drop variables with near zero variance
  step_nzv(all_numeric(), -all_outcomes()) %>%
  # Standardize all numeric variables
  step_normalize(all_numeric(), -all_outcomes())

office_rec
```

To check how data will look like after the recipe is applied we first need to prepare the recipe and then we need to "juice" it. Prepping means to calculate the steps on the given data (e.g. calculating the mean and sd for normalizing, etc.), and juicing means to actually do it on the data.

```{r}
# This block is not necessary for doing the analysis.
# Calculating the pre-processing steps
office_prep <- 
  office_rec %>%
  prep()

# Checking the pre-processed data
juice(office_prep)
```

## Create the model specification

We use a linear regression model. The `mixture` defines that we will use a LASSO model (1 = LASSO, 0 = ridge). The `penalty` is the lambda parameter. In this analysis, we use a fixed value, but in more advanced analysis, we would try several values (hyperparameter tuning).

```{r}
lasso_spec <- 
  linear_reg(penalty = 0.0233, mixture = 1) %>%
    set_engine("glmnet")
```

In the `{tidymodels}` framework, the pre-processing, the model specification, the data, and other things are put together into a workflow. We can use this workflow.

```{r}
office_wf <- 
  workflow() %>%
  add_recipe(office_rec) %>%
  add_model(lasso_spec)

# Fitting the model
lasso_fit <- 
  office_wf %>%
  fit(data = office_train)
```

We can check the whole iterative process of including predictors at each step. 

```{r}
lasso_steps <-
  lasso_fit %>%
  extract_fit_engine() %>% 
  tidy() %>% 
  arrange(step)

lasso_steps %>% 
  add_count(term) %>% 
  mutate(term = fct_reorder(term, n)) %>% 
  ggplot() +
  aes(x = step, y = term, fill = estimate, alpha = lambda) +
  geom_tile(show.legend = FALSE) +
  scale_y_discrete(name = NULL)
```

Showing the predictors that remain in the final model. 

```{r}
lasso_fit %>%
  extract_fit_parsnip() %>% 
  tidy() %>% 
  filter(estimate > 0) %>% 
  arrange(-estimate)
```

## Variable importance

It is also possible to tell the importance of each variable by calculating the relative importance score for all variables. 

```{r}
library(vip)

lasso_fit %>%
  extract_fit_parsnip() %>%
  vi() %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

## Evaluating the model

The final model evaluation has to be done on the test set. If the results on the test set are too different from the training set, we would know that we overfitted the data.

```{r}
last_fit(lasso_fit, office_split) %>%
  collect_metrics()
```
